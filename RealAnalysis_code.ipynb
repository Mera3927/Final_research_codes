{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install openml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T21:21:48.335054Z","iopub.execute_input":"2025-03-01T21:21:48.335331Z","iopub.status.idle":"2025-03-01T21:21:55.027088Z","shell.execute_reply.started":"2025-03-01T21:21:48.335309Z","shell.execute_reply":"2025-03-01T21:21:55.026237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_diabetes, fetch_california_housing\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.ensemble import StackingRegressor, StackingClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.inspection import permutation_importance\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport warnings\nimport openml\nimport time","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Regression","metadata":{}},{"cell_type":"code","source":"# Suppress warnings\nwarnings.filterwarnings('ignore')\n\ndef compute_stacking_feature_importance(X, y, X_test, y_test, base_models, meta_model):\n    \"\"\"\n    Your original method to compute feature importance for stacking model\n    \"\"\"\n    # Generate predictions from base models\n    base_train_preds = np.column_stack([\n        model.predict(X) for model in base_models\n    ])\n\n    base_test_preds = np.column_stack([\n        model.predict(X_test) for model in base_models\n    ])\n\n    # Train meta-model\n    meta_model.fit(base_train_preds, y)\n\n    # Get meta-model weights\n    meta_model_weights = np.abs(meta_model.coef_)\n    meta_model_weights = meta_model_weights / np.sum(meta_model_weights)\n\n    # Get feature importance from base models\n    base_importances = []\n    for model in base_models:\n        importance = model.feature_importances_\n        importance = importance / np.sum(importance)\n        base_importances.append(importance)\n    base_importances = np.array(base_importances)\n\n    # Calculate stability adjustment\n    importance_std = np.std(base_importances, axis=0)  # Standard deviation across models for each feature\n    epsilon = 1  # Small constant to avoid division by zero\n    stability_adjustment = 1 / (importance_std + epsilon)\n\n    # Normalize stability adjustment\n    stability_adjustment = stability_adjustment / np.sum(stability_adjustment)\n\n    # Calculate final feature importance\n    overall_importance = np.zeros(X.shape[1])\n    for i, importance in enumerate(base_importances):\n        overall_importance += importance * meta_model_weights[i]\n\n    # Apply stability adjustment to overall importance\n    adjusted_importance = overall_importance * stability_adjustment\n\n    # Ensure normalization\n    adjusted_importance = adjusted_importance / np.sum(adjusted_importance)\n\n    #overall_importance = overall_importance / np.sum(overall_importance)\n    \n    return adjusted_importance\n\ndef compute_permutation_feature_importance(X, y, X_test, y_test, base_models, meta_model):\n    \"\"\"\n    Compute permutation feature importance for stacking model using StackingRegressor\n    \"\"\"\n    # Create named estimators for StackingRegressor\n    named_estimators = [\n        ('xgb', base_models[0]),\n        ('lgb', base_models[1]),\n        ('rf', base_models[2]),\n        ('et', base_models[3]),\n        ('dt', base_models[4]),\n        ('ada', base_models[5])\n    ]\n    \n    # Create StackingRegressor with no CV (passthrough=False to avoid including original features)\n    stacking_model = StackingRegressor(\n        estimators=named_estimators,\n        final_estimator=meta_model,\n        cv=None,  # No cross-validation\n        passthrough=False\n    )\n    \n    # Train the stacking model\n    stacking_model.fit(X, y)\n    \n    # Compute permutation importance\n    result = permutation_importance(\n        estimator=stacking_model, \n        X=X_test,\n        y=y_test,\n        n_repeats=10,\n        random_state=42\n    )\n    \n    # Normalize importance scores\n    perm_importance = result.importances_mean\n    perm_importance = perm_importance / np.sum(perm_importance)\n    \n    return perm_importance\n\ndef load_dataset(dataset_name):\n    \"\"\"Load dataset by name\"\"\"\n    if dataset_name == \"california\":\n        data = fetch_california_housing()\n        X, y = data.data, data.target\n        feature_names = data.feature_names\n    elif dataset_name == \"diabetes\":\n        data = load_diabetes()\n        X, y = data.data, data.target\n        feature_names = data.feature_names\n    elif dataset_name == \"concrete\":\n        # Concrete Compressive Strength from OpenML\n        dataset = openml.datasets.get_dataset(44959)\n        data = dataset.get_data(dataset_format=\"dataframe\")\n        X = data[0].drop('strength', axis=1)\n        y = data[0]['strength']\n        feature_names = X.columns.tolist()\n        X = X.values\n    elif dataset_name == \"air\":\n        # Energy Efficiency from OpenML (heating load)\n        dataset = openml.datasets.get_dataset(547)\n        data = dataset.get_data(dataset_format=\"dataframe\")\n        X = data[0].drop(['no2_concentration'], axis=1)\n        y = data[0]['no2_concentration']  # Heating Load\n        feature_names = X.columns.tolist()\n        X = X.values\n    elif dataset_name == \"water\":\n        !kaggle datasets download -d devanshibavaria/water-potability-dataset-with-10-parameteres\n        !unzip -q water-potability-dataset-with-10-parameteres.zip\n        df = pd.read_csv(\"water_potability.csv\")\n        X = df.drop('Potability', axis=1)  # Features (10 parameters)\n        y = df['Potability']               # Target (binary: 0/1)\n        X = X.fillna(X.mean())\n        X = X.values\n        feature_names = df.drop('Potability', axis=1).columns.tolist()\n    else:\n        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n    \n    return X, y, feature_names\n\ndef compare_feature_importance_methods(dataset_name):\n    \"\"\"Compare your method with permutation feature importance\"\"\"\n    print(f\"\\nAnalyzing dataset: {dataset_name}\")\n    \n    # Load dataset\n    X, y, feature_names = load_dataset(dataset_name)\n    \n    # Scale features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n    \n    # Initialize models\n    base_models = [\n        xgb.XGBRegressor(random_state=42),\n        lgb.LGBMRegressor(random_state=42, verbose=-1),\n        RandomForestRegressor(random_state=42),\n        ExtraTreesRegressor(random_state=42),\n        DecisionTreeRegressor(random_state=42),\n        AdaBoostRegressor(random_state=42)\n    ]\n    \n    meta_model = Ridge()\n    \n    # Train base models\n    for model in base_models:\n        model.fit(X_train, y_train)\n    \n    # Time and compute method3 \n    start_time = time.time()\n    method3_importance = compute_stacking_feature_importance(\n        X_train, y_train, X_test, y_test, base_models, meta_model\n    )\n    method3_method_time = time.time() - start_time\n    \n    # Time and compute permutation importance with StackingRegressor\n    start_time = time.time()\n    perm_importance = compute_permutation_feature_importance(\n        X_train, y_train, X_test, y_test, base_models, meta_model\n    )\n    perm_method_time = time.time() - start_time\n    \n    # Prepare and return results\n    comparison_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Method 3': your_importance,\n        'Permutation Importance': perm_importance\n    })\n    \n    # Sort by method 3's importance\n    comparison_df = comparison_df.sort_values('Method 3', ascending=False)\n    \n    # Calculate correlation between methods\n    correlation = np.corrcoef(your_importance, perm_importance)[0, 1]\n    \n    # Create StackingRegressor for performance evaluation\n    named_estimators = [\n        ('xgb', base_models[0]),\n        ('lgb', base_models[1]),\n        ('rf', base_models[2]),\n        ('et', base_models[3]),\n        ('dt', base_models[4]),\n        ('ada', base_models[5])\n    ]\n    \n    stacking_model = StackingRegressor(\n        estimators=named_estimators,\n        final_estimator=meta_model,\n        cv=None,\n        passthrough=False\n    )\n    \n    # Base model performance metrics\n    model_scores = {}\n    for i, model in enumerate(base_models):\n        model_name = model.__class__.__name__\n        y_pred = model.predict(X_test)\n        mse = mean_squared_error(y_test, y_pred)\n        r2 = r2_score(y_test, y_pred)\n        model_scores[model_name] = {'MSE': mse, 'R2': r2}\n    \n    # Stacking model performance\n    stacking_model.fit(X_train, y_train)\n    stacking_pred = stacking_model.predict(X_test)\n    stacking_mse = mean_squared_error(y_test, stacking_pred)\n    stacking_r2 = r2_score(y_test, stacking_pred)\n    model_scores['Stacking'] = {'MSE': stacking_mse, 'R2': stacking_r2}\n    \n    return {\n        'comparison': comparison_df,\n        'correlation': correlation,\n        'method3_time': your_method_time,\n        'perm_method_time': perm_method_time,\n        'model_scores': model_scores\n    }\n\ndef visualize_comparison(results, dataset_name):\n    \"\"\"Visualize comparison between methods\"\"\"\n    comparison_df = results['comparison']\n    \n    plt.figure(figsize=(12, 8))\n    \n    # Side by side bar plots\n    x = np.arange(len(comparison_df))\n    width = 0.35\n    \n    plt.bar(x - width/2, comparison_df['Method 3'], width, label='Method 3')\n    plt.bar(x + width/2, comparison_df['Permutation Importance'], width, label='Permutation Importance')\n    \n    plt.xlabel('Features')\n    plt.ylabel('Importance (normalized)')\n    plt.title(f'Feature Importance Comparison - {dataset_name.title()} Dataset\\nCorrelation: {results[\"correlation\"]:.4f}')\n    plt.xticks(x, comparison_df['Feature'], rotation=90)\n    plt.legend()\n    plt.tight_layout()\n    \n    return plt.gcf()\n\ndef analyze_all_datasets():\n    \"\"\"Run analysis on all datasets\"\"\"\n    datasets = [\"california\", \"diabetes\", \"air\", \"concrete\"]\n    all_results = {}\n    summary_rows = []\n    \n    for dataset in datasets:\n        try:\n            results = compare_feature_importance_methods(dataset)\n            all_results[dataset] = results\n            \n            # Create visualization\n            fig = visualize_comparison(results, dataset)\n            plt.close(fig)\n            \n            # Top features by each method\n            top_yours = results['comparison'].nlargest(3, 'Method 3')['Feature'].tolist()\n            top_perm = results['comparison'].nlargest(3, 'Permutation Importance')['Feature'].tolist()\n            \n            # Add to summary\n            summary_rows.append({\n                'Dataset': dataset.title(),\n                'Correlation': results['correlation'],\n                'Method 3 Time (s)': results['method3_time'],\n                'Permutation Time (s)': results['perm_method_time'],\n                'Top-3 Features (Method 3)': ', '.join(top_yours),\n                'Top-3 Features (Perm)': ', '.join(top_perm),\n                'Stacking R2': results['model_scores']['Stacking']['R2'],\n                'Speedup Factor': results['perm_method_time'] / results['method3_time']\n            })\n        except Exception as e:\n            print(f\"Error processing dataset {dataset}: {str(e)}\")\n    \n    # Create summary table\n    summary_df = pd.DataFrame(summary_rows)\n    \n    return all_results, summary_df\n\n# Run analysis on all datasets\nall_results, summary_df = analyze_all_datasets()\n\n# Print summary\nprint(\"\\n===== SUMMARY OF RESULTS =====\")\nprint(summary_df.to_string(index=False))\n\n# Generate detailed comparison for each dataset\nfor dataset_name, results in all_results.items():\n    print(f\"\\n\\n===== DETAILED RESULTS FOR {dataset_name.upper()} =====\")\n    print(\"\\nFeature Importance Comparison:\")\n    print(results['comparison'].to_string(index=False))\n    \n    print(f\"\\nCorrelation between methods: {results['correlation']:.4f}\")\n    print(f\"method 3 execution time: {results['method3_time']:.4f} seconds\")\n    print(f\"Permutation importance execution time: {results['perm_method_time']:.4f} seconds\")\n    print(f\"Speedup factor: {results['perm_method_time'] / results['method3_time']:.2f}x\")\n    \n    print(\"\\nModel Performance:\")\n    model_df = pd.DataFrame.from_dict(results['model_scores'], orient='index')\n    print(model_df.to_string())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-01T21:21:57.999134Z","iopub.execute_input":"2025-03-01T21:21:57.999493Z","execution_failed":"2025-03-02T06:19:38.783Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"# Suppress warnings\nwarnings.filterwarnings('ignore')\n\ndef compute_stacking_feature_importance(X, y, X_test, y_test, base_models, meta_model):\n    \"\"\"\n    Compute feature importance for stacking classification model\n    \"\"\"\n    # Generate predictions from base models\n    base_train_preds = np.column_stack([\n        model.predict_proba(X)[:, 1] if hasattr(model, 'predict_proba') else model.predict(X)\n        for model in base_models\n    ])\n\n    base_test_preds = np.column_stack([\n        model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.predict(X_test)\n        for model in base_models\n    ])\n\n    # Train meta-model\n    meta_model.fit(base_train_preds, y)\n\n    # Get meta-model weights (use absolute values for importance)\n    if hasattr(meta_model, 'coef_'):\n        meta_model_weights = np.abs(meta_model.coef_[0])\n    else:\n        meta_model_weights = np.ones(len(base_models))\n    meta_model_weights = meta_model_weights / np.sum(meta_model_weights)\n\n    # Get feature importance from base models\n    base_importances = []\n    for model in base_models:\n        if hasattr(model, 'feature_importances_'):\n            importance = model.feature_importances_\n        elif hasattr(model, 'coef_'):\n            importance = np.abs(model.coef_[0])\n        else:\n            importance = np.ones(X.shape[1])\n        importance = importance / np.sum(importance)\n        base_importances.append(importance)\n    base_importances = np.array(base_importances)\n\n    # Calculate stability adjustment\n    importance_std = np.std(base_importances, axis=0)  # Standard deviation across models for each feature\n    epsilon = 1  # Small constant to avoid division by zero\n    stability_adjustment = 1 / (importance_std + epsilon)\n\n    # Normalize stability adjustment\n    stability_adjustment = stability_adjustment / np.sum(stability_adjustment)\n\n    # Calculate final feature importance\n    overall_importance = np.zeros(X.shape[1])\n    for i, importance in enumerate(base_importances):\n        overall_importance += importance * meta_model_weights[i]\n\n    # Apply stability adjustment to overall importance\n    adjusted_importance = overall_importance * stability_adjustment\n\n    # Ensure normalization\n    adjusted_importance = adjusted_importance / np.sum(adjusted_importance)\n\n    #overall_importance = overall_importance / np.sum(overall_importance)\n    \n    return adjusted_importance\n\ndef compute_permutation_feature_importance(X, y, X_test, y_test, base_models, meta_model):\n    \"\"\"\n    Compute permutation feature importance for stacking classification model\n    \"\"\"\n    # Create named estimators for StackingClassifier\n    named_estimators = [\n        ('xgb', base_models[0]),\n        ('lgb', base_models[1]),\n        ('rf', base_models[2]),\n        ('et', base_models[3]),\n        ('dt', base_models[4]),\n        ('ada', base_models[5])\n    ]\n    \n    # Create StackingClassifier with no CV (passthrough=False to avoid including original features)\n    stacking_model = StackingClassifier(\n        estimators=named_estimators,\n        final_estimator=meta_model,\n        cv=None,  # No cross-validation\n        passthrough=False\n    )\n    \n    # Train the stacking model\n    stacking_model.fit(X, y)\n    \n    # Compute permutation importance\n    result = permutation_importance(\n        estimator=stacking_model, \n        X=X_test,\n        y=y_test,\n        n_repeats=10,\n        random_state=42\n    )\n    \n    # Normalize importance scores\n    perm_importance = result.importances_mean\n    perm_importance = perm_importance / np.sum(perm_importance)\n    \n    return perm_importance\n\ndef load_dataset(dataset_name):\n    \"\"\"Load dataset by name\"\"\"\n    if dataset_name == \"california\":\n        data = fetch_california_housing()\n        X, y = data.data, (data.target > data.target.mean()).astype(int)  # Binary classification\n        feature_names = data.feature_names\n    elif dataset_name == \"diabetes\":\n        data = load_diabetes()\n        X, y = data.data, (data.target > data.target.mean()).astype(int)  # Binary classification\n        feature_names = data.feature_names\n    elif dataset_name == \"concrete\":\n        # Concrete Compressive Strength from OpenML\n        dataset = openml.datasets.get_dataset(44959)\n        data = dataset.get_data(dataset_format=\"dataframe\")\n        X = data[0].drop('strength', axis=1)\n        y = (data[0]['strength'] > data[0]['strength'].mean()).astype(int)  # Binary classification\n        feature_names = X.columns.tolist()\n        X = X.values\n    elif dataset_name == \"parkinson\":\n        # Parkinson's dataset from OpenML\n        dataset = openml.datasets.get_dataset(1488)\n        data = dataset.get_data(dataset_format=\"dataframe\")\n        X = data[0].drop(['Class'], axis=1)\n        y = data[0]['Class']  # Already categorical\n\n        y_labels = data[0]['Class']\n        label_map = {label: i for i, label in enumerate(y_labels.unique())}\n        y = y_labels.map(label_map)\n        \n        feature_names = X.columns.tolist()\n        X = X.values\n    elif dataset_name == \"air\":\n        # Air Quality dataset from OpenML\n        dataset = openml.datasets.get_dataset(547)\n        data = dataset.get_data(dataset_format=\"dataframe\")\n        X = data[0].drop(['no2_concentration'], axis=1)\n        y = (data[0]['no2_concentration'] > data[0]['no2_concentration'].mean()).astype(int)  # Binary classification\n        feature_names = X.columns.tolist()\n        X = X.values\n    elif dataset_name == \"water\":\n        # Water Potability dataset (already binary classification)\n        !kaggle datasets download -d devanshibavaria/water-potability-dataset-with-10-parameteres\n        !unzip -q water-potability-dataset-with-10-parameteres.zip\n        df = pd.read_csv(\"water_potability.csv\")\n        X = df.drop('Potability', axis=1)  # Features (10 parameters)\n        y = df['Potability']               # Target (binary: 0/1)\n        X = X.fillna(X.mean())\n        X = X.values\n        feature_names = df.drop('Potability', axis=1).columns.tolist()\n    else:\n        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n    \n    return X, y, feature_names\n\ndef compare_feature_importance_methods(dataset_name):\n    \"\"\"Compare your method with permutation feature importance for classification\"\"\"\n    print(f\"\\nAnalyzing dataset: {dataset_name}\")\n    \n    # Load dataset\n    X, y, feature_names = load_dataset(dataset_name)\n    \n    # Scale features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n    \n    # Initialize models\n    base_models = [\n        xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n        lgb.LGBMClassifier(random_state=42, verbose=-1),\n        RandomForestClassifier(random_state=42),\n        ExtraTreesClassifier(random_state=42),\n        DecisionTreeClassifier(random_state=42),\n        AdaBoostClassifier(random_state=42)\n    ]\n    \n    meta_model = LogisticRegression(random_state=42)\n    \n    # Train base models\n    for model in base_models:\n        model.fit(X_train, y_train)\n    \n    # Time and compute your method\n    start_time = time.time()\n    your_importance = compute_stacking_feature_importance(\n        X_train, y_train, X_test, y_test, base_models, meta_model\n    )\n    your_method_time = time.time() - start_time\n    \n    # Time and compute permutation importance with StackingClassifier\n    start_time = time.time()\n    perm_importance = compute_permutation_feature_importance(\n        X_train, y_train, X_test, y_test, base_models, meta_model\n    )\n    perm_method_time = time.time() - start_time\n    \n    # Prepare and return results\n    comparison_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Your Method': your_importance,\n        'Permutation Importance': perm_importance\n    })\n    \n    # Sort by your method's importance\n    comparison_df = comparison_df.sort_values('Your Method', ascending=False)\n    \n    # Calculate correlation between methods\n    correlation = np.corrcoef(your_importance, perm_importance)[0, 1]\n    \n    # Create StackingClassifier for performance evaluation\n    named_estimators = [\n        ('xgb', base_models[0]),\n        ('lgb', base_models[1]),\n        ('rf', base_models[2]),\n        ('et', base_models[3]),\n        ('dt', base_models[4]),\n        ('ada', base_models[5])\n    ]\n    \n    stacking_model = StackingClassifier(\n        estimators=named_estimators,\n        final_estimator=meta_model,\n        cv=None,\n        passthrough=False\n    )\n    \n    # Base model performance metrics\n    model_scores = {}\n    for i, model in enumerate(base_models):\n        model_name = model.__class__.__name__\n        y_pred = model.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred, average='weighted')\n        model_scores[model_name] = {'Accuracy': accuracy, 'F1 Score': f1}\n    \n    # Stacking model performance\n    stacking_model.fit(X_train, y_train)\n    stacking_pred = stacking_model.predict(X_test)\n    stacking_accuracy = accuracy_score(y_test, stacking_pred)\n    stacking_f1 = f1_score(y_test, stacking_pred, average='weighted')\n    model_scores['Stacking'] = {'Accuracy': stacking_accuracy, 'F1 Score': stacking_f1}\n    \n    return {\n        'comparison': comparison_df,\n        'correlation': correlation,\n        'your_method_time': your_method_time,\n        'perm_method_time': perm_method_time,\n        'model_scores': model_scores\n    }\n\ndef visualize_comparison(results, dataset_name):\n    \"\"\"Visualize comparison between methods\"\"\"\n    comparison_df = results['comparison']\n    \n    plt.figure(figsize=(12, 8))\n    \n    # Side by side bar plots\n    x = np.arange(len(comparison_df))\n    width = 0.35\n    \n    plt.bar(x - width/2, comparison_df['Your Method'], width, label='Your Method')\n    plt.bar(x + width/2, comparison_df['Permutation Importance'], width, label='Permutation Importance')\n    \n    plt.xlabel('Features')\n    plt.ylabel('Importance (normalized)')\n    plt.title(f'Feature Importance Comparison - {dataset_name.title()} Dataset\\nCorrelation: {results[\"correlation\"]:.4f}')\n    plt.xticks(x, comparison_df['Feature'], rotation=90)\n    plt.legend()\n    plt.tight_layout()\n    \n    return plt.gcf()\n\ndef analyze_all_datasets():\n    \"\"\"Run analysis on all datasets\"\"\"\n    datasets = [\"water\"]\n    all_results = {}\n    summary_rows = []\n    \n    for dataset in datasets:\n        try:\n            results = compare_feature_importance_methods(dataset)\n            all_results[dataset] = results\n            \n            # Create visualization\n            fig = visualize_comparison(results, dataset)\n            plt.close(fig)\n            \n            # Top features by each method\n            top_yours = results['comparison'].nlargest(3, 'Your Method')['Feature'].tolist()\n            top_perm = results['comparison'].nlargest(3, 'Permutation Importance')['Feature'].tolist()\n            \n            # Add to summary\n            summary_rows.append({\n                'Dataset': dataset.title(),\n                'Correlation': results['correlation'],\n                'Your Method Time (s)': results['your_method_time'],\n                'Permutation Time (s)': results['perm_method_time'],\n                'Top-3 Features (Your)': ', '.join(top_yours),\n                'Top-3 Features (Perm)': ', '.join(top_perm),\n                'Stacking Accuracy': results['model_scores']['Stacking']['Accuracy'],\n                'Stacking F1 Score': results['model_scores']['Stacking']['F1 Score'],\n                'Speedup Factor': results['perm_method_time'] / results['your_method_time']\n            })\n        except Exception as e:\n            print(f\"Error processing dataset {dataset}: {str(e)}\")\n    \n    # Create summary table\n    summary_df = pd.DataFrame(summary_rows)\n    \n    return all_results, summary_df\n\n# Run analysis on all datasets\nall_results, summary_df = analyze_all_datasets()\n\n# Print summary\nprint(\"\\n===== SUMMARY OF RESULTS =====\")\nprint(summary_df.to_string(index=False))\n\n# Generate detailed comparison for each dataset\nfor dataset_name, results in all_results.items():\n    print(f\"\\n\\n===== DETAILED RESULTS FOR {dataset_name.upper()} =====\")\n    print(\"\\nFeature Importance Comparison:\")\n    print(results['comparison'].to_string(index=False))\n    \n    print(f\"\\nCorrelation between methods: {results['correlation']:.4f}\")\n    print(f\"Your method execution time: {results['your_method_time']:.4f} seconds\")\n    print(f\"Permutation importance execution time: {results['perm_method_time']:.4f} seconds\")\n    print(f\"Speedup factor: {results['perm_method_time'] / results['your_method_time']:.2f}x\")\n    \n    print(\"\\nModel Performance:\")\n    model_df = pd.DataFrame.from_dict(results['model_scores'], orient='index')\n    print(model_df.to_string())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}