{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning G:\\UOC\\Level 4\\Research\\Outputs\\Method 2\\Regression\\simulation_results for simulation results...\n",
      "Found 112 valid scenario folders\n",
      "\n",
      "Processing 6 scenarios with 3 features and weight type EQ...\n",
      "\n",
      "Processing 6 scenarios with 3 features and weight type GRAD_DEC...\n",
      "\n",
      "Processing 6 scenarios with 3 features and weight type HIGH_LOW...\n",
      "\n",
      "Processing 6 scenarios with 3 features and weight type ONE_DOM...\n",
      "\n",
      "Processing 6 scenarios with 3 features and weight type TWO_DOM...\n",
      "\n",
      "Processing 6 scenarios with 5 features and weight type EQ...\n",
      "\n",
      "Processing 6 scenarios with 5 features and weight type GRAD_DEC...\n",
      "\n",
      "Processing 6 scenarios with 5 features and weight type HIGH_LOW...\n",
      "\n",
      "Processing 5 scenarios with 5 features and weight type ONE_DOM...\n",
      "\n",
      "Processing 5 scenarios with 5 features and weight type TWO_DOM...\n",
      "\n",
      "Processing 5 scenarios with 7 features and weight type EQ...\n",
      "\n",
      "Processing 5 scenarios with 7 features and weight type GRAD_DEC...\n",
      "\n",
      "Processing 5 scenarios with 7 features and weight type HIGH_LOW...\n",
      "\n",
      "Processing 5 scenarios with 7 features and weight type ONE_DOM...\n",
      "\n",
      "Processing 5 scenarios with 7 features and weight type TWO_DOM...\n",
      "\n",
      "Processing 6 scenarios with 10 features and weight type EQ...\n",
      "\n",
      "Processing 6 scenarios with 10 features and weight type GRAD_DEC...\n",
      "\n",
      "Processing 5 scenarios with 10 features and weight type HIGH_LOW...\n",
      "\n",
      "Processing 6 scenarios with 10 features and weight type ONE_DOM...\n",
      "\n",
      "Processing 6 scenarios with 10 features and weight type TWO_DOM...\n",
      "Feature importance summary saved to feature_analysis_summaries_regression_02\\feature_importance_summary.csv\n",
      "Meta weights summary saved to feature_analysis_summaries_regression_02\\meta_weights_summary.csv\n",
      "Rank correlation summary saved to feature_analysis_summaries_regression_02\\rank_correlation_summary.csv\n",
      "Mean absolute deviation summary saved to feature_analysis_summaries_regression_02\\mad_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def extract_scenario_info(scenario_id):\n",
    "    \"\"\"Extract information from scenario ID\"\"\"\n",
    "    # Pattern: SIM{n_sims}_N{sample_size}_F{n_features}_W{weight_type}\n",
    "    pattern = r'SIM(\\d+)_N(\\d+)_F(\\d+)_W(.+)'\n",
    "    match = re.match(pattern, scenario_id)\n",
    "    if match:\n",
    "        n_sims, sample_size, n_features, weight_type = match.groups()\n",
    "        return {\n",
    "            'n_simulations': int(n_sims),\n",
    "            'sample_size': int(sample_size),\n",
    "            'n_features': int(n_features),\n",
    "            'weight_type': weight_type\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def summarize_by_feature_count_and_weight(master_folder, output_folder=\"feature_weight_summaries\"):\n",
    "    \"\"\"Summarize simulation results by feature count and weight distribution\"\"\"\n",
    "    \n",
    "    # Create output folder if it doesn't exist\n",
    "    output_path = Path(output_folder)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(f\"Scanning {master_folder} for simulation results...\")\n",
    "    \n",
    "    # Find all scenario folders\n",
    "    all_scenarios = []\n",
    "    for folder_path in Path(master_folder).glob(\"**/SIM*\"):\n",
    "        if folder_path.is_dir():\n",
    "            scenario_id = folder_path.name\n",
    "            scenario_info = extract_scenario_info(scenario_id)\n",
    "            \n",
    "            if scenario_info:\n",
    "                scenario_info['folder_path'] = folder_path\n",
    "                scenario_info['scenario_id'] = scenario_id\n",
    "                all_scenarios.append(scenario_info)\n",
    "    \n",
    "    print(f\"Found {len(all_scenarios)} valid scenario folders\")\n",
    "    \n",
    "    # Group scenarios by feature count and weight type\n",
    "    feature_weight_groups = {}\n",
    "    for scenario in all_scenarios:\n",
    "        n_features = scenario['n_features']\n",
    "        weight_type = scenario['weight_type']\n",
    "        key = (n_features, weight_type)\n",
    "        \n",
    "        if key not in feature_weight_groups:\n",
    "            feature_weight_groups[key] = []\n",
    "        feature_weight_groups[key].append(scenario)\n",
    "    \n",
    "    # Summary dataframes\n",
    "    feature_importance_summary = []\n",
    "    accuracy_summary = []\n",
    "    meta_weights_summary = []\n",
    "    correlation_summary = []  # New summary for rank correlation\n",
    "    mad_summary = []  # New summary for mean absolute deviation\n",
    "    \n",
    "    # Process each feature count and weight type group\n",
    "    for (n_features, weight_type), scenarios in sorted(feature_weight_groups.items()):\n",
    "        print(f\"\\nProcessing {len(scenarios)} scenarios with {n_features} features and weight type {weight_type}...\")\n",
    "        \n",
    "        # Initialize aggregated data\n",
    "        all_importances = []\n",
    "        all_accuracies = []\n",
    "        all_meta_weights = []\n",
    "        \n",
    "        for scenario in scenarios:\n",
    "            folder_path = scenario['folder_path']\n",
    "            scenario_id = scenario['scenario_id']\n",
    "            \n",
    "            # Find feature importance CSV\n",
    "            imp_files = list(folder_path.glob(\"*feature_importances.csv\"))\n",
    "            if imp_files:\n",
    "                try:\n",
    "                    # Read feature importance data\n",
    "                    imp_df = pd.read_csv(imp_files[0])\n",
    "                    \n",
    "                    # Add scenario metadata\n",
    "                    imp_df['scenario_id'] = scenario_id\n",
    "                    imp_df['n_features'] = n_features\n",
    "                    imp_df['weight_type'] = weight_type\n",
    "                    imp_df['sample_size'] = scenario['sample_size']\n",
    "                    imp_df['n_simulations'] = scenario['n_simulations']\n",
    "                    \n",
    "                    all_importances.append(imp_df)\n",
    "                    \n",
    "                    # Read accuracy data\n",
    "                    acc_files = list(folder_path.glob(\"*accuracies.csv\"))\n",
    "                    if acc_files:\n",
    "                        acc_df = pd.read_csv(acc_files[0])\n",
    "                        acc_df['scenario_id'] = scenario_id\n",
    "                        acc_df['n_features'] = n_features\n",
    "                        acc_df['weight_type'] = weight_type\n",
    "                        acc_df['sample_size'] = scenario['sample_size']\n",
    "                        acc_df['n_simulations'] = scenario['n_simulations']\n",
    "                        all_accuracies.append(acc_df)\n",
    "                    \n",
    "                    # Read meta weights data\n",
    "                    meta_files = list(folder_path.glob(\"*meta_weights.csv\"))\n",
    "                    if meta_files:\n",
    "                        meta_df = pd.read_csv(meta_files[0])\n",
    "                        meta_df['scenario_id'] = scenario_id\n",
    "                        meta_df['n_features'] = n_features\n",
    "                        meta_df['weight_type'] = weight_type\n",
    "                        meta_df['sample_size'] = scenario['sample_size']\n",
    "                        meta_df['n_simulations'] = scenario['n_simulations']\n",
    "                        all_meta_weights.append(meta_df)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {scenario_id}: {str(e)}\")\n",
    "        \n",
    "        if all_importances:\n",
    "            # Combine all data\n",
    "            combined_importances = pd.concat(all_importances, ignore_index=True)\n",
    "            combined_accuracies = pd.concat(all_accuracies, ignore_index=True) if all_accuracies else None\n",
    "            combined_meta_weights = pd.concat(all_meta_weights, ignore_index=True) if all_meta_weights else None\n",
    "            \n",
    "            # Calculate feature importance summaries\n",
    "            importance_stats = summarize_feature_importance(combined_importances, n_features, weight_type)\n",
    "            feature_importance_summary.append(importance_stats)\n",
    "            \n",
    "            # Calculate accuracy summaries\n",
    "            if combined_accuracies is not None:\n",
    "                accuracy_stats = summarize_accuracies(combined_accuracies, n_features, weight_type)\n",
    "                accuracy_summary.append(accuracy_stats)\n",
    "            \n",
    "            # Calculate meta-weight summaries\n",
    "            if combined_meta_weights is not None:\n",
    "                meta_stats = summarize_meta_weights(combined_meta_weights, n_features, weight_type)\n",
    "                meta_weights_summary.append(meta_stats)\n",
    "            \n",
    "            # Calculate rank correlation and MAD with true weights\n",
    "            if combined_importances is not None:\n",
    "                corr_stats = calculate_rank_correlations(combined_importances, n_features, weight_type)\n",
    "                correlation_summary.append(corr_stats)\n",
    "                \n",
    "                mad_stats = calculate_mean_absolute_deviations(combined_importances, n_features, weight_type)\n",
    "                mad_summary.append(mad_stats)\n",
    "            \n",
    "            # Generate visualizations\n",
    "            create_visualizations(combined_importances, combined_accuracies, \n",
    "                                combined_meta_weights, n_features, weight_type, output_path)\n",
    "    \n",
    "    # Combine all summaries\n",
    "    if feature_importance_summary:\n",
    "        fi_summary_df = pd.concat(feature_importance_summary, ignore_index=True)\n",
    "        fi_summary_df.to_csv(output_path / \"feature_importance_summary.csv\", index=False)\n",
    "        print(f\"Feature importance summary saved to {output_path / 'feature_importance_summary.csv'}\")\n",
    "    \n",
    "    if accuracy_summary:\n",
    "        acc_summary_df = pd.concat(accuracy_summary, ignore_index=True)\n",
    "        acc_summary_df.to_csv(output_path / \"accuracy_summary.csv\", index=False)\n",
    "        print(f\"Accuracy summary saved to {output_path / 'accuracy_summary.csv'}\")\n",
    "    \n",
    "    if meta_weights_summary:\n",
    "        meta_summary_df = pd.concat(meta_weights_summary, ignore_index=True)\n",
    "        meta_summary_df.to_csv(output_path / \"meta_weights_summary.csv\", index=False)\n",
    "        print(f\"Meta weights summary saved to {output_path / 'meta_weights_summary.csv'}\")\n",
    "    \n",
    "    if correlation_summary:\n",
    "        corr_summary_df = pd.concat(correlation_summary, ignore_index=True)\n",
    "        corr_summary_df.to_csv(output_path / \"rank_correlation_summary.csv\", index=False)\n",
    "        print(f\"Rank correlation summary saved to {output_path / 'rank_correlation_summary.csv'}\")\n",
    "    \n",
    "    if mad_summary:\n",
    "        mad_summary_df = pd.concat(mad_summary, ignore_index=True)\n",
    "        mad_summary_df.to_csv(output_path / \"mad_summary.csv\", index=False)\n",
    "        print(f\"Mean absolute deviation summary saved to {output_path / 'mad_summary.csv'}\")\n",
    "    \n",
    "    # Create overall comparison across feature counts and weight types\n",
    "    if feature_importance_summary and accuracy_summary:\n",
    "        create_feature_weight_comparison(fi_summary_df, acc_summary_df, \n",
    "                                     meta_summary_df if meta_weights_summary else None,\n",
    "                                     corr_summary_df if correlation_summary else None,\n",
    "                                     mad_summary_df if mad_summary else None,\n",
    "                                     output_path)\n",
    "\n",
    "def summarize_feature_importance(df, n_features, weight_type):\n",
    "    \"\"\"Calculate summary statistics for feature importance\"\"\"\n",
    "    models = ['true_weights', 'xgboost', 'lgbm', 'random_forest', \n",
    "              'extra_trees', 'decision_tree', 'ada_boost', 'stacking']\n",
    "    \n",
    "    result_rows = []\n",
    "    \n",
    "    # For each model\n",
    "    for model in models:\n",
    "        model_cols = [col for col in df.columns if model in col and 'feature' in col]\n",
    "        \n",
    "        if not model_cols:\n",
    "            continue\n",
    "            \n",
    "        # For each feature\n",
    "        for feature_idx in range(1, n_features + 1):\n",
    "            feature_col = f\"{model}_feature_{feature_idx}\"\n",
    "            \n",
    "            if feature_col not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            # Calculate statistics\n",
    "            mean_value = df[feature_col].mean()\n",
    "            median_value = df[feature_col].median()\n",
    "            std_value = df[feature_col].std()\n",
    "            var_value = df[feature_col].var()\n",
    "            min_value = df[feature_col].min()\n",
    "            max_value = df[feature_col].max()\n",
    "            q1_value = df[feature_col].quantile(0.25)\n",
    "            q3_value = df[feature_col].quantile(0.75)\n",
    "            \n",
    "            # Add row to results\n",
    "            result_rows.append({\n",
    "                'n_features': n_features,\n",
    "                'weight_type': weight_type,\n",
    "                'model': model,\n",
    "                'feature': feature_idx,\n",
    "                'mean': mean_value,\n",
    "                'median': median_value,\n",
    "                'std': std_value,\n",
    "                'variance': var_value,\n",
    "                'min': min_value,\n",
    "                'max': max_value,\n",
    "                'q1': q1_value,\n",
    "                'q3': q3_value\n",
    "            })\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    return pd.DataFrame(result_rows)\n",
    "\n",
    "def summarize_accuracies(df, n_features, weight_type):\n",
    "    \"\"\"Calculate summary statistics for model accuracies\"\"\"\n",
    "    result_rows = []\n",
    "    \n",
    "    # Calculate statistics for each model\n",
    "    for model in df.columns:\n",
    "        if model in ['scenario_id', 'n_features', 'weight_type', 'sample_size', 'n_simulations', 'Unnamed: 0']:\n",
    "            continue\n",
    "            \n",
    "        # Calculate statistics\n",
    "        mean_value = df[model].mean()\n",
    "        median_value = df[model].median()\n",
    "        std_value = df[model].std()\n",
    "        var_value = df[model].var()\n",
    "        min_value = df[model].min()\n",
    "        max_value = df[model].max()\n",
    "        q1_value = df[model].quantile(0.25)\n",
    "        q3_value = df[model].quantile(0.75)\n",
    "        \n",
    "        # Add row to results\n",
    "        result_rows.append({\n",
    "            'n_features': n_features,\n",
    "            'weight_type': weight_type,\n",
    "            'model': model,\n",
    "            'mean_accuracy': mean_value,\n",
    "            'median_accuracy': median_value,\n",
    "            'std_accuracy': std_value,\n",
    "            'variance_accuracy': var_value,\n",
    "            'min_accuracy': min_value,\n",
    "            'max_accuracy': max_value,\n",
    "            'q1_accuracy': q1_value,\n",
    "            'q3_accuracy': q3_value\n",
    "        })\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    return pd.DataFrame(result_rows)\n",
    "\n",
    "def summarize_meta_weights(df, n_features, weight_type):\n",
    "    \"\"\"Calculate summary statistics for meta-model weights\"\"\"\n",
    "    result_rows = []\n",
    "    \n",
    "    # Calculate statistics for each base model weight\n",
    "    for col_idx in range(6):  # 6 base models\n",
    "        if col_idx >= df.shape[1]:\n",
    "            continue\n",
    "            \n",
    "        col = str(col_idx)\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Map column index to model name\n",
    "        models = ['xgboost', 'lgbm', 'random_forest', 'extra_trees', 'decision_tree', 'ada_boost']\n",
    "        model_name = models[col_idx] if col_idx < len(models) else f\"model_{col_idx}\"\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean_value = df[col].mean()\n",
    "        median_value = df[col].median()\n",
    "        std_value = df[col].std()\n",
    "        var_value = df[col].var()\n",
    "        min_value = df[col].min()\n",
    "        max_value = df[col].max()\n",
    "        q1_value = df[col].quantile(0.25)\n",
    "        q3_value = df[col].quantile(0.75)\n",
    "        \n",
    "        # Add row to results\n",
    "        result_rows.append({\n",
    "            'n_features': n_features,\n",
    "            'weight_type': weight_type,\n",
    "            'base_model': model_name,\n",
    "            'mean_weight': mean_value,\n",
    "            'median_weight': median_value,\n",
    "            'std_weight': std_value,\n",
    "            'variance_weight': var_value,\n",
    "            'min_weight': min_value,\n",
    "            'max_weight': max_value,\n",
    "            'q1_weight': q1_value,\n",
    "            'q3_weight': q3_value\n",
    "        })\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    return pd.DataFrame(result_rows)\n",
    "\n",
    "def calculate_rank_correlations(df, n_features, weight_type):\n",
    "    \"\"\"Calculate rank correlation between model feature importance and true weights using Kendall's Tau\"\"\"\n",
    "    from scipy.stats import kendalltau\n",
    "    \n",
    "    models = ['xgboost', 'lgbm', 'random_forest', \n",
    "              'extra_trees', 'decision_tree', 'ada_boost', 'stacking']\n",
    "    \n",
    "    result_rows = []\n",
    "    \n",
    "    # Iterate through each simulation/row\n",
    "    for idx in range(len(df)):\n",
    "        row = df.iloc[idx]\n",
    "        \n",
    "        # Get true weights for this simulation\n",
    "        true_weights = []\n",
    "        for feature_idx in range(1, n_features + 1):\n",
    "            true_col = f\"true_weights_feature_{feature_idx}\"\n",
    "            if true_col in df.columns:\n",
    "                true_weights.append(row[true_col])\n",
    "        \n",
    "        # Calculate Kendall's Tau for each model\n",
    "        for model in models:\n",
    "            model_weights = []\n",
    "            for feature_idx in range(1, n_features + 1):\n",
    "                model_col = f\"{model}_feature_{feature_idx}\"\n",
    "                if model_col in df.columns:\n",
    "                    model_weights.append(row[model_col])\n",
    "            \n",
    "            # Only calculate if we have data for both\n",
    "            if true_weights and model_weights and len(true_weights) == len(model_weights):\n",
    "                try:\n",
    "                    # Calculate Kendall's Tau\n",
    "                    tau, p_value = kendalltau(true_weights, model_weights)\n",
    "                    \n",
    "                    result_rows.append({\n",
    "                        'n_features': n_features,\n",
    "                        'weight_type': weight_type,\n",
    "                        'model': model,\n",
    "                        'sim_idx': idx,\n",
    "                        'rank_correlation': tau,\n",
    "                        'p_value': p_value\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating Kendall's Tau: {str(e)}\")\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    corr_df = pd.DataFrame(result_rows)\n",
    "    \n",
    "    # Calculate summary statistics for Kendall's Tau\n",
    "    summary_rows = []\n",
    "    for model in models:\n",
    "        model_corrs = corr_df[corr_df['model'] == model]\n",
    "        \n",
    "        if len(model_corrs) > 0:\n",
    "            summary_rows.append({\n",
    "                'n_features': n_features,\n",
    "                'weight_type': weight_type,\n",
    "                'model': model,\n",
    "                'mean_rank_corr': model_corrs['rank_correlation'].mean(),\n",
    "                'median_rank_corr': model_corrs['rank_correlation'].median(),\n",
    "                'std_rank_corr': model_corrs['rank_correlation'].std(),\n",
    "                'min_rank_corr': model_corrs['rank_correlation'].min(),\n",
    "                'max_rank_corr': model_corrs['rank_correlation'].max()\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(summary_rows)\n",
    "\n",
    "def calculate_mean_absolute_deviations(df, n_features, weight_type):\n",
    "    \"\"\"Calculate mean absolute deviation between model feature importance and true weights\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    models = ['xgboost', 'lgbm', 'random_forest', \n",
    "              'extra_trees', 'decision_tree', 'ada_boost', 'stacking']\n",
    "    \n",
    "    result_rows = []\n",
    "    \n",
    "    # Iterate through each simulation/row\n",
    "    for idx in range(len(df)):\n",
    "        row = df.iloc[idx]\n",
    "        \n",
    "        # Get true weights for this simulation\n",
    "        true_weights = []\n",
    "        for feature_idx in range(1, n_features + 1):\n",
    "            true_col = f\"true_weights_feature_{feature_idx}\"\n",
    "            if true_col in df.columns:\n",
    "                true_weights.append(row[true_col])\n",
    "        \n",
    "        # Calculate MAD for each model\n",
    "        for model in models:\n",
    "            model_weights = []\n",
    "            for feature_idx in range(1, n_features + 1):\n",
    "                model_col = f\"{model}_feature_{feature_idx}\"\n",
    "                if model_col in df.columns:\n",
    "                    model_weights.append(row[model_col])\n",
    "            \n",
    "            # Only calculate if we have data for both\n",
    "            if true_weights and model_weights and len(true_weights) == len(model_weights):\n",
    "                try:\n",
    "                    # Calculate Mean Absolute Deviation\n",
    "                    mad = np.mean(np.abs(np.array(true_weights) - np.array(model_weights)))\n",
    "                    \n",
    "                    result_rows.append({\n",
    "                        'n_features': n_features,\n",
    "                        'weight_type': weight_type,\n",
    "                        'model': model,\n",
    "                        'sim_idx': idx,\n",
    "                        'mean_abs_deviation': mad\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating MAD: {str(e)}\")\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    mad_df = pd.DataFrame(result_rows)\n",
    "    \n",
    "    # Calculate summary statistics for MADs\n",
    "    summary_rows = []\n",
    "    for model in models:\n",
    "        model_mads = mad_df[mad_df['model'] == model]\n",
    "        \n",
    "        if len(model_mads) > 0:\n",
    "            summary_rows.append({\n",
    "                'n_features': n_features,\n",
    "                'weight_type': weight_type,\n",
    "                'model': model,\n",
    "                'mean_mad': model_mads['mean_abs_deviation'].mean(),\n",
    "                'median_mad': model_mads['mean_abs_deviation'].median(),\n",
    "                'std_mad': model_mads['mean_abs_deviation'].std(),\n",
    "                'min_mad': model_mads['mean_abs_deviation'].min(),\n",
    "                'max_mad': model_mads['mean_abs_deviation'].max()\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(summary_rows)\n",
    "\n",
    "def create_visualizations(imp_df, acc_df, meta_df, n_features, weight_type, output_path):\n",
    "    \"\"\"Create visualization plots for each feature count and weight type\"\"\"\n",
    "    feature_dir = output_path / f\"features_{n_features}_weights_{weight_type}\"\n",
    "    feature_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # 1. Feature Importance Comparison\n",
    "    try:\n",
    "        models = ['true_weights', 'xgboost', 'lgbm', 'random_forest', \n",
    "                'extra_trees', 'decision_tree', 'ada_boost', 'stacking']\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        plot_data = []\n",
    "        for model in models:\n",
    "            for feature_idx in range(1, n_features + 1):\n",
    "                col_name = f\"{model}_feature_{feature_idx}\"\n",
    "                if col_name in imp_df.columns:\n",
    "                    for value in imp_df[col_name]:\n",
    "                        plot_data.append({\n",
    "                            'Model': model,\n",
    "                            'Feature': f\"Feature {feature_idx}\",\n",
    "                            'Importance': value\n",
    "                        })\n",
    "        \n",
    "        if plot_data:\n",
    "            plot_df = pd.DataFrame(plot_data)\n",
    "            \n",
    "            plt.figure(figsize=(15, 10))\n",
    "            sns.boxplot(data=plot_df, x='Feature', y='Importance', hue='Model')\n",
    "            plt.title(f'Feature Importance Distribution ({n_features} Features, {weight_type} Weights)')\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.grid(True, linestyle='--', alpha=0.5)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(feature_dir / f\"importance_boxplot_{n_features}_{weight_type}.png\", dpi=300)\n",
    "            plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating feature importance visualization: {str(e)}\")\n",
    "    \n",
    "    # 2. Accuracy Comparison\n",
    "    if acc_df is not None:\n",
    "        try:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            acc_models = [col for col in acc_df.columns \n",
    "                        if col not in ['scenario_id', 'n_features', 'weight_type', \n",
    "                                      'sample_size', 'n_simulations', 'Unnamed: 0']]\n",
    "            sns.boxplot(data=acc_df[acc_models])\n",
    "            plt.title(f'Model Accuracy Comparison ({n_features} Features, {weight_type} Weights)')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.grid(True, linestyle='--', alpha=0.5)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(feature_dir / f\"accuracy_boxplot_{n_features}_{weight_type}.png\", dpi=300)\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating accuracy visualization: {str(e)}\")\n",
    "    \n",
    "    # 3. Meta Weights Comparison\n",
    "    if meta_df is not None:\n",
    "        try:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            meta_cols = [col for col in meta_df.columns \n",
    "                        if col not in ['scenario_id', 'n_features', 'weight_type', \n",
    "                                      'sample_size', 'n_simulations', 'Unnamed: 0']]\n",
    "            \n",
    "            # Map column indices to model names\n",
    "            models = ['xgboost', 'lgbm', 'random_forest', 'extra_trees', 'decision_tree', 'ada_boost']\n",
    "            rename_dict = {str(i): models[i] for i in range(len(models)) if str(i) in meta_cols}\n",
    "            \n",
    "            if rename_dict:\n",
    "                meta_plot_df = meta_df[meta_cols].rename(columns=rename_dict)\n",
    "                sns.boxplot(data=meta_plot_df)\n",
    "                plt.title(f'Meta-Model Weights Distribution ({n_features} Features, {weight_type} Weights)')\n",
    "                plt.ylabel('Weight')\n",
    "                plt.grid(True, linestyle='--', alpha=0.5)\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(feature_dir / f\"meta_weights_boxplot_{n_features}_{weight_type}.png\", dpi=300)\n",
    "                plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating meta weights visualization: {str(e)}\")\n",
    "    \n",
    "    # 4. Rank Correlation Visualization\n",
    "    try:\n",
    "        # Calculate rank correlations for this specific dataset\n",
    "        rank_corrs = calculate_rank_correlations(imp_df, n_features, weight_type)\n",
    "        \n",
    "        if not rank_corrs.empty:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.barplot(data=rank_corrs, x='model', y='mean_rank_corr')\n",
    "            plt.title(f'Mean Rank Correlation with True Weights ({n_features} Features, {weight_type} Weights)')\n",
    "            plt.ylabel('Kendals Rank Correlation')\n",
    "            plt.xlabel('Model')\n",
    "            plt.grid(True, linestyle='--', alpha=0.5)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(feature_dir / f\"rank_correlation_{n_features}_{weight_type}.png\", dpi=300)\n",
    "            plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating rank correlation visualization: {str(e)}\")\n",
    "    \n",
    "    # 5. Mean Absolute Deviation Visualization\n",
    "    try:\n",
    "        # Calculate MADs for this specific dataset\n",
    "        mad_stats = calculate_mean_absolute_deviations(imp_df, n_features, weight_type)\n",
    "        \n",
    "        if not mad_stats.empty:\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.barplot(data=mad_stats, x='model', y='mean_mad')\n",
    "            plt.title(f'Mean Absolute Deviation from True Weights ({n_features} Features, {weight_type} Weights)')\n",
    "            plt.ylabel('Mean Absolute Deviation')\n",
    "            plt.xlabel('Model')\n",
    "            plt.grid(True, linestyle='--', alpha=0.5)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(feature_dir / f\"mean_abs_deviation_{n_features}_{weight_type}.png\", dpi=300)\n",
    "            plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating MAD visualization: {str(e)}\")\n",
    "\n",
    "def create_feature_weight_comparison(fi_df, acc_df, meta_df, corr_df, mad_df, output_path):\n",
    "    \"\"\"Create comparison visualizations across feature counts and weight types for Stacking\"\"\"\n",
    "    \n",
    "    # Focus only on Stacking model results\n",
    "    stacking_acc_df = acc_df[acc_df['model'] == 'stacking']\n",
    "    stacking_corr_df = corr_df[corr_df['model'] == 'stacking']\n",
    "    stacking_mad_df = mad_df[mad_df['model'] == 'stacking']\n",
    "    \n",
    "    # 1. Stacking Accuracy Heatmap\n",
    "    try:\n",
    "        # Create pivot table for stacking accuracy\n",
    "        pivot_acc_data = stacking_acc_df.pivot_table(\n",
    "            index='n_features',\n",
    "            columns='weight_type',\n",
    "            values='mean_accuracy',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(pivot_acc_data, annot=True, cmap='viridis', fmt='.4f', \n",
    "                    cbar_kws={'label': 'Stacking Accuracy'})\n",
    "        plt.title('Stacking Accuracy by Feature Count and Weight Type')\n",
    "        plt.xlabel('Weight Type')\n",
    "        plt.ylabel('Number of Features')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path / \"stacking_accuracy_heatmap.png\", dpi=300)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating stacking accuracy heatmap: {str(e)}\")\n",
    "    \n",
    "    # 2. Stacking Rank Correlation Heatmap\n",
    "    if not stacking_corr_df.empty:\n",
    "        try:\n",
    "            # Create pivot table for stacking rank correlation\n",
    "            pivot_corr_data = stacking_corr_df.pivot_table(\n",
    "                index='n_features',\n",
    "                columns='weight_type',\n",
    "                values='mean_rank_corr',\n",
    "                aggfunc='mean'\n",
    "            )\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(pivot_corr_data, annot=True, cmap='coolwarm', fmt='.4f', center=0,\n",
    "                        cbar_kws={'label': 'Rank Correlation'})\n",
    "            plt.title('Stacking Rank Correlation by Feature Count and Weight Type')\n",
    "            plt.xlabel('Weight Type')\n",
    "            plt.ylabel('Number of Features')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(output_path / \"stacking_rank_correlation_heatmap.png\", dpi=300)\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating stacking rank correlation heatmap: {str(e)}\")\n",
    "    \n",
    "    # 3. Stacking Mean Absolute Deviation Heatmap\n",
    "    if not stacking_mad_df.empty:\n",
    "        try:\n",
    "            # Create pivot table for stacking MAD\n",
    "            pivot_mad_data = stacking_mad_df.pivot_table(\n",
    "                index='n_features',\n",
    "                columns='weight_type',\n",
    "                values='mean_mad',\n",
    "                aggfunc='mean'\n",
    "            )\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(pivot_mad_data, annot=True, cmap='YlOrRd_r', fmt='.4f',\n",
    "                        cbar_kws={'label': 'Mean Absolute Deviation'})\n",
    "            plt.title('Stacking Mean Absolute Deviation by Feature Count and Weight Type')\n",
    "            plt.xlabel('Weight Type')\n",
    "            plt.ylabel('Number of Features')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(output_path / \"stacking_mean_absolute_deviation_heatmap.png\", dpi=300)\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating stacking MAD heatmap: {str(e)}\")\n",
    "            \n",
    "    # Generate comprehensive summary report\n",
    "    generate_summary_report(fi_df, acc_df, meta_df, corr_df, mad_df, output_path)\n",
    "\n",
    "def generate_summary_report(fi_df, acc_df, meta_df, corr_df, mad_df, output_path):\n",
    "    \"\"\"Generate a detailed text report summarizing all findings\"\"\"\n",
    "    report_path = output_path / \"feature_weight_summary_report.txt\"\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"SIMULATION STUDY SUMMARY REPORT BY FEATURE COUNT AND WEIGHT TYPE\\n\")\n",
    "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "        \n",
    "        # Overview\n",
    "        unique_features = sorted(fi_df['n_features'].unique())\n",
    "        unique_weights = sorted(fi_df['weight_type'].unique())\n",
    "        f.write(f\"Analysis covers {len(unique_features)} feature counts: {', '.join(map(str, unique_features))}\\n\")\n",
    "        f.write(f\"Analysis covers {len(unique_weights)} weight types: {', '.join(map(str, unique_weights))}\\n\\n\")\n",
    "        \n",
    "        # Part 1: Accuracy Summary\n",
    "        f.write(\"PART 1: MODEL ACCURACY SUMMARY\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        # Best model by feature count and weight type\n",
    "        f.write(\"Best Performing Models by Feature Count and Weight Type:\\n\")\n",
    "        for n_features in unique_features:\n",
    "            for weight_type in unique_weights:\n",
    "                subset = acc_df[(acc_df['n_features'] == n_features) & \n",
    "                                (acc_df['weight_type'] == weight_type)]\n",
    "                \n",
    "                if not subset.empty:\n",
    "                    best_model = subset.loc[subset['mean_accuracy'].idxmax()]\n",
    "                    f.write(f\"  Features: {n_features}, Weight Type: {weight_type} - Best Model: {best_model['model']} \")\n",
    "                    f.write(f\"(Accuracy: {best_model['mean_accuracy']:.4f} Â± {best_model['std_accuracy']:.4f})\\n\")\n",
    "        \n",
    "        f.write(\"\\nOverall Best Performing Models (Averaged across scenarios):\\n\")\n",
    "        model_avg = acc_df.groupby('model')['mean_accuracy'].mean().sort_values(ascending=False)\n",
    "        for model, avg_acc in model_avg.items():\n",
    "            f.write(f\"  {model}: {avg_acc:.4f}\\n\")\n",
    "        \n",
    "        # Part 2: Feature Importance Summary\n",
    "        f.write(\"\\n\\nPART 2: FEATURE IMPORTANCE SUMMARY\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        # Rank correlation summary\n",
    "        if corr_df is not None and not corr_df.empty:\n",
    "            f.write(\"Feature Importance Rank Correlation with True Weights:\\n\")\n",
    "            for n_features in unique_features:\n",
    "                for weight_type in unique_weights:\n",
    "                    subset = corr_df[(corr_df['n_features'] == n_features) & \n",
    "                                    (corr_df['weight_type'] == weight_type)]\n",
    "                    \n",
    "                    if not subset.empty:\n",
    "                        f.write(f\"\\n  Features: {n_features}, Weight Type: {weight_type}\\n\")\n",
    "                        \n",
    "                        # Sort models by rank correlation\n",
    "                        sorted_models = subset.sort_values('mean_rank_corr', ascending=False)\n",
    "                        for _, row in sorted_models.iterrows():\n",
    "                            f.write(f\"    {row['model']}: {row['mean_rank_corr']:.4f} \")\n",
    "                            f.write(f\"(Range: {row['min_rank_corr']:.4f} to {row['max_rank_corr']:.4f})\\n\")\n",
    "        \n",
    "        # Mean Absolute Deviation summary\n",
    "        if mad_df is not None and not mad_df.empty:\n",
    "            f.write(\"\\nMean Absolute Deviation from True Weights:\\n\")\n",
    "            for n_features in unique_features:\n",
    "                for weight_type in unique_weights:\n",
    "                    subset = mad_df[(mad_df['n_features'] == n_features) & \n",
    "                                   (mad_df['weight_type'] == weight_type)]\n",
    "                    \n",
    "                    if not subset.empty:\n",
    "                        f.write(f\"\\n  Features: {n_features}, Weight Type: {weight_type}\\n\")\n",
    "                        \n",
    "                        # Sort models by MAD (ascending - lower is better)\n",
    "                        sorted_models = subset.sort_values('mean_mad', ascending=True)\n",
    "                        for _, row in sorted_models.iterrows():\n",
    "                            f.write(f\"    {row['model']}: {row['mean_mad']:.4f} \")\n",
    "                            f.write(f\"(Range: {row['min_mad']:.4f} to {row['max_mad']:.4f})\\n\")\n",
    "        \n",
    "        # Part 3: Meta-Model Weights Summary\n",
    "        if meta_df is not None and not meta_df.empty:\n",
    "            f.write(\"\\n\\nPART 3: META-MODEL WEIGHTS SUMMARY\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(\"Distribution of Meta-Model Weights by Feature Count and Weight Type:\\n\")\n",
    "            for n_features in unique_features:\n",
    "                for weight_type in unique_weights:\n",
    "                    subset = meta_df[(meta_df['n_features'] == n_features) & \n",
    "                                    (meta_df['weight_type'] == weight_type)]\n",
    "                    \n",
    "                    if not subset.empty:\n",
    "                        f.write(f\"\\n  Features: {n_features}, Weight Type: {weight_type}\\n\")\n",
    "                        \n",
    "                        # Sort base models by mean weight\n",
    "                        sorted_models = subset.sort_values('mean_weight', ascending=False)\n",
    "                        for _, row in sorted_models.iterrows():\n",
    "                            f.write(f\"    {row['base_model']}: {row['mean_weight']:.4f} \")\n",
    "                            f.write(f\"(Range: {row['min_weight']:.4f} to {row['max_weight']:.4f})\\n\")\n",
    "        \n",
    "        # Part 4: Key Findings and Recommendations\n",
    "        f.write(\"\\n\\nPART 4: KEY FINDINGS AND RECOMMENDATIONS\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        # Identify overall best model\n",
    "        overall_best_model = acc_df.groupby('model')['mean_accuracy'].mean().idxmax()\n",
    "        overall_best_acc = acc_df.groupby('model')['mean_accuracy'].mean().max()\n",
    "        \n",
    "        f.write(f\"1. Overall Best Performing Model: {overall_best_model} (Average Accuracy: {overall_best_acc:.4f})\\n\")\n",
    "        \n",
    "        # Identify best model for feature importance ranking\n",
    "        if corr_df is not None and not corr_df.empty:\n",
    "            best_ranking_model = corr_df.groupby('model')['mean_rank_corr'].mean().idxmax()\n",
    "            best_ranking_corr = corr_df.groupby('model')['mean_rank_corr'].mean().max()\n",
    "            f.write(f\"2. Best Model for Feature Importance Ranking: {best_ranking_model} \")\n",
    "            f.write(f\"(Average Rank Correlation: {best_ranking_corr:.4f})\\n\")\n",
    "        \n",
    "        # Best model for different feature counts\n",
    "        f.write(\"3. Best Models by Feature Count:\\n\")\n",
    "        for n_features in unique_features:\n",
    "            subset = acc_df[acc_df['n_features'] == n_features]\n",
    "            if not subset.empty:\n",
    "                best_model = subset.groupby('model')['mean_accuracy'].mean().idxmax()\n",
    "                best_acc = subset.groupby('model')['mean_accuracy'].mean().max()\n",
    "                f.write(f\"   - {n_features} Features: {best_model} (Average Accuracy: {best_acc:.4f})\\n\")\n",
    "        \n",
    "        # Best model for different weight types\n",
    "        f.write(\"4. Best Models by Weight Type:\\n\")\n",
    "        for weight_type in unique_weights:\n",
    "            subset = acc_df[acc_df['weight_type'] == weight_type]\n",
    "            if not subset.empty:\n",
    "                best_model = subset.groupby('model')['mean_accuracy'].mean().idxmax()\n",
    "                best_acc = subset.groupby('model')['mean_accuracy'].mean().max()\n",
    "                f.write(f\"   - {weight_type} Weights: {best_model} (Average Accuracy: {best_acc:.4f})\\n\")\n",
    "        \n",
    "        # Additional findings\n",
    "        f.write(\"\\nAdditional Observations:\\n\")\n",
    "        \n",
    "        # Feature count impact\n",
    "        f.write(\"- Impact of Feature Count on Performance:\\n\")\n",
    "        feature_impact = acc_df.groupby('n_features')['mean_accuracy'].mean()\n",
    "        for n_features, mean_acc in feature_impact.items():\n",
    "            f.write(f\"  {n_features} Features: {mean_acc:.4f}\\n\")\n",
    "        \n",
    "        # Weight type impact\n",
    "        f.write(\"- Impact of Weight Type on Performance:\\n\")\n",
    "        weight_impact = acc_df.groupby('weight_type')['mean_accuracy'].mean()\n",
    "        for weight_type, mean_acc in weight_impact.items():\n",
    "            f.write(f\"  {weight_type} Weights: {mean_acc:.4f}\\n\")\n",
    "        \n",
    "        # Recommendations\n",
    "        f.write(\"\\nRecommendations:\\n\")\n",
    "        f.write(f\"1. For optimal predictive performance, the {overall_best_model} model is recommended.\\n\")\n",
    "        \n",
    "        if corr_df is not None and not corr_df.empty:\n",
    "            f.write(f\"2. For feature importance interpretation, the {best_ranking_model} model provides \")\n",
    "            f.write(\"the most reliable feature rankings.\\n\")\n",
    "        \n",
    "        f.write(\"3. Meta-model (stacking) performance suggests that combining models \")\n",
    "        f.write(\"may provide more stable results across different scenarios.\\n\")\n",
    "        \n",
    "        f.write(\"\\nReport generated on: \" + datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\\n\")\n",
    "    \n",
    "    print(f\"Summary report generated: {report_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your master folder path\n",
    "    master_folder = \"G:\\\\UOC\\\\Level 4\\\\Research\\\\Outputs\\\\Method 2\\\\Classification\\\\simulation_results\"\n",
    "    output_folder = \"feature_analysis_summaries_classification_02\"\n",
    "    \n",
    "    # Run the summary analysis\n",
    "    summarize_by_feature_count_and_weight(master_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
