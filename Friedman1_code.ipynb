{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import itertools\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom pathlib import Path\nimport multiprocessing as mp\nfrom functools import partial\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.gridspec import GridSpec\nimport warnings\nimport logging\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.datasets import make_friedman1, make_friedman2, make_friedman3\nfrom scipy.stats import kendalltau\n\n# Suppress all warnings\nwarnings.filterwarnings('ignore')\n\n# Configure logging\nlogging.basicConfig(level=logging.WARNING)\nlogging.getLogger('xgboost').setLevel(logging.ERROR)\nlogging.getLogger('lightgbm').setLevel(logging.ERROR)\n\nclass SimulationConfig:\n    def __init__(self):\n        self.n_simulations_options = [100]  # Fixed number of simulations\n        self.sample_sizes = [1000, 2500]  # Sample sizes\n        self.n_features_options = [7, 10]  # Feature counts to compare\n        self.friedman_types = [\"friedman1\"]  # Using only one type for simplicity\n        # Define meta-models for different methods\n        self.meta_models = {\n            'method1': LinearRegression(),\n            'method2': Ridge(),\n            'method3': Ridge(),\n            'method4': Ridge()\n        }\n\ndef generate_friedman_data(n_samples, n_features, friedman_type, random_state=None):\n    \"\"\"Generate Friedman dataset based on type\"\"\"\n    if friedman_type == \"friedman1\":\n        X, y = make_friedman1(n_samples=n_samples, n_features=n_features, random_state=random_state)\n\n    else:\n        raise ValueError(f\"Unknown Friedman type: {friedman_type}\")\n    \n    return X, y\n\ndef compute_stacking_feature_importance_method1(X, y, X_test, y_test, base_models, meta_model):\n    \"\"\"Compute feature importance for stacking model - Method 1 (Linear Regression)\"\"\"\n    # Generate predictions from base models\n    base_train_preds = np.column_stack([\n        model.predict(X) for model in base_models\n    ])\n\n    base_test_preds = np.column_stack([\n        model.predict(X_test) for model in base_models\n    ])\n\n    # Train meta-model\n    meta_model.fit(base_train_preds, y)\n\n    # Get meta-model weights\n    if hasattr(meta_model, 'coef_'):\n        meta_model_weights = np.abs(meta_model.coef_)\n    else:\n        meta_model_weights = np.ones(len(base_models)) / len(base_models)\n    meta_model_weights = meta_model_weights / np.sum(meta_model_weights)\n\n    # Get feature importance from base models\n    base_importances = []\n    for model in base_models:\n        importance = model.feature_importances_\n        importance = importance / np.sum(importance)\n        base_importances.append(importance)\n    base_importances = np.array(base_importances)\n\n    # Calculate final feature importance\n    overall_importance = np.zeros(X.shape[1])\n    for i, importance in enumerate(base_importances):\n        overall_importance += importance * meta_model_weights[i]\n\n    # Ensure normalization\n    overall_importance = overall_importance / np.sum(overall_importance)\n    \n    # Get predictions\n    stacking_pred = meta_model.predict(base_test_preds)\n\n    return overall_importance, meta_model_weights, meta_model, stacking_pred\n\ndef compute_stacking_feature_importance_method2(X, y, X_test, y_test, base_models, meta_model):\n    \"\"\"Compute feature importance for stacking model - Method 2 (Ridge Regression)\"\"\"\n    # Generate predictions from base models\n    base_train_preds = np.column_stack([\n        model.predict(X) for model in base_models\n    ])\n\n    base_test_preds = np.column_stack([\n        model.predict(X_test) for model in base_models\n    ])\n\n    # Train meta-model\n    meta_model.fit(base_train_preds, y)\n\n    # Get meta-model weights\n    if hasattr(meta_model, 'coef_'):\n        meta_model_weights = np.abs(meta_model.coef_)\n    else:\n        meta_model_weights = np.ones(len(base_models)) / len(base_models)\n    meta_model_weights = meta_model_weights / np.sum(meta_model_weights)\n\n    # Get feature importance from base models\n    base_importances = []\n    for model in base_models:\n        importance = model.feature_importances_\n        importance = importance / np.sum(importance)\n        base_importances.append(importance)\n    base_importances = np.array(base_importances)\n\n    # Calculate final feature importance\n    overall_importance = np.zeros(X.shape[1])\n    for i, importance in enumerate(base_importances):\n        overall_importance += importance * meta_model_weights[i]\n\n    # Ensure normalization\n    overall_importance = overall_importance / np.sum(overall_importance)\n    \n    # Get predictions\n    stacking_pred = meta_model.predict(base_test_preds)\n\n    return overall_importance, meta_model_weights, meta_model, stacking_pred\n\ndef compute_stacking_feature_importance_method3(X, y, X_test, y_test, base_models, meta_model):\n    \"\"\"Compute feature importance for stacking model with stability adjustment\"\"\"\n    # Generate predictions from base models\n    base_train_preds = np.column_stack([\n        model.predict(X) for model in base_models\n    ])\n\n    base_test_preds = np.column_stack([\n        model.predict(X_test) for model in base_models\n    ])\n\n    # Train meta-model\n    meta_model.fit(base_train_preds, y)\n\n    # Get meta-model weights\n    if hasattr(meta_model, 'coef_'):\n        meta_model_weights = np.abs(meta_model.coef_)\n    else:\n        meta_model_weights = np.ones(len(base_models)) / len(base_models)\n    meta_model_weights = meta_model_weights / np.sum(meta_model_weights)\n\n    # Get feature importance from base models\n    base_importances = []\n    for model in base_models:\n        importance = model.feature_importances_\n        importance = importance / np.sum(importance)\n        base_importances.append(importance)\n    base_importances = np.array(base_importances)\n\n    # Calculate stability adjustment\n    importance_std = np.std(base_importances, axis=0)  # Standard deviation across models for each feature\n    epsilon = 1  # Small constant to avoid division by zero\n    stability_adjustment = 1 / (importance_std + epsilon)\n\n    # Normalize stability adjustment\n    stability_adjustment = stability_adjustment / np.sum(stability_adjustment)\n\n    # Calculate final feature importance\n    overall_importance = np.zeros(X.shape[1])\n    for i, importance in enumerate(base_importances):\n        overall_importance += importance * meta_model_weights[i]\n\n    # Apply stability adjustment to overall importance\n    adjusted_importance = overall_importance * stability_adjustment\n\n    # Ensure normalization\n    adjusted_importance = adjusted_importance / np.sum(adjusted_importance)\n    \n    # Get predictions\n    stacking_pred = meta_model.predict(base_test_preds)\n\n    return adjusted_importance, meta_model_weights, meta_model, stacking_pred\n\ndef compute_stacking_feature_importance_method4(X, y, X_test, y_test, base_models, meta_model):\n    \"\"\"Compute feature importance for stacking model with Kendall's Tau penalty\"\"\"\n    # Generate predictions from base models\n    base_train_preds = np.column_stack([\n        model.predict(X) for model in base_models\n    ])\n\n    base_test_preds = np.column_stack([\n        model.predict(X_test) for model in base_models\n    ])\n\n    # Train meta-model\n    meta_model.fit(base_train_preds, y)\n\n    # Get meta-model weights\n    if hasattr(meta_model, 'coef_'):\n        meta_model_weights = np.abs(meta_model.coef_)\n    else:\n        meta_model_weights = np.ones(len(base_models)) / len(base_models)\n    meta_model_weights = meta_model_weights / np.sum(meta_model_weights)\n\n    # Get feature importance from base models\n    base_importances = []\n    for model in base_models:\n        importance = model.feature_importances_\n        importance = importance / np.sum(importance)\n        base_importances.append(importance)\n    base_importances = np.array(base_importances)\n\n    # Calculate Kendall's Tau for each pair of models to assess ranking consistency\n    n_models = len(base_models)\n    tau_matrix = np.zeros((n_models, n_models))\n    \n    for i in range(n_models):\n        for j in range(i+1, n_models):\n            # Calculate Kendall's Tau for feature importance rankings between models\n            tau, _ = kendalltau(base_importances[i], base_importances[j])\n            # Convert from [-1,1] range to [0,1] range where 1 means perfect agreement\n            tau_norm = (tau + 1) / 2\n            tau_matrix[i, j] = tau_norm\n            tau_matrix[j, i] = tau_norm\n    \n    # Calculate average Kendall's Tau for each model (row-wise mean)\n    avg_tau = np.mean(tau_matrix, axis=1)\n    \n    # Convert average Tau to penalty factor (higher tau → higher penalty)\n    # Use inverse relationship: 2 - tau to make higher tau values have higher penalties\n    tau_penalty = 2 - avg_tau\n    \n    # Apply tau penalty to meta model weights\n    adjusted_meta_weights = meta_model_weights * tau_penalty\n    adjusted_meta_weights = adjusted_meta_weights / np.sum(adjusted_meta_weights)\n\n    # Calculate final feature importance using penalized weights\n    overall_importance = np.zeros(X.shape[1])\n    for i, importance in enumerate(base_importances):\n        overall_importance += importance * adjusted_meta_weights[i]\n\n    # Ensure normalization of final importance values\n    adjusted_importance = overall_importance / np.sum(overall_importance)\n    \n    # Get predictions\n    stacking_pred = meta_model.predict(base_test_preds)\n\n    return adjusted_importance, adjusted_meta_weights, meta_model, stacking_pred\n\ndef run_simulation(n_simulations, n_samples, n_features, friedman_type, config):\n    \"\"\"Run simulation with given parameters\"\"\"\n    # Store results for each method\n    methods_fi_results = {\n        'method1': np.zeros((n_simulations, n_features)),\n        'method2': np.zeros((n_simulations, n_features)),\n        'method3': np.zeros((n_simulations, n_features)),\n        'method4': np.zeros((n_simulations, n_features))\n    }\n    \n    # Store meta model weights\n    methods_meta_weights = {\n        'method1': np.zeros((n_simulations, 6)),\n        'method2': np.zeros((n_simulations, 6)),\n        'method3': np.zeros((n_simulations, 6)),\n        'method4': np.zeros((n_simulations, 6))\n    }\n    \n    # Store model performance metrics\n    methods_scores = {\n        'method1': {'r2': [], 'mse': []},\n        'method2': {'r2': [], 'mse': []},\n        'method3': {'r2': [], 'mse': []},\n        'method4': {'r2': [], 'mse': []}\n    }\n\n    for i in range(n_simulations):\n        # Generate data from Friedman dataset\n        X, y = generate_friedman_data(n_samples, n_features, friedman_type, random_state=i)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n\n        # Initialize base models\n        base_models = [\n            xgb.XGBRegressor(random_state=i),\n            lgb.LGBMRegressor(random_state=i, verbose=-1),\n            RandomForestRegressor(random_state=i),\n            ExtraTreesRegressor(random_state=i),\n            DecisionTreeRegressor(random_state=i),\n            AdaBoostRegressor(random_state=i)\n        ]\n        \n        # Train base models\n        for model in base_models:\n            model.fit(X_train, y_train)\n        \n        # Define methods with their specific feature importance and meta-model\n        methods = {\n            'method1': (compute_stacking_feature_importance_method1, config.meta_models['method1']),\n            'method2': (compute_stacking_feature_importance_method2, config.meta_models['method2']),\n            'method3': (compute_stacking_feature_importance_method3, config.meta_models['method3']),\n            'method4': (compute_stacking_feature_importance_method4, config.meta_models['method4'])\n        }\n        \n        # Compute feature importance for each method\n        for method_name, (method_func, meta_model) in methods.items():\n            # Compute stacking feature importance\n            stacking_output = method_func(\n                X_train, y_train, X_test, y_test, base_models, meta_model\n            )\n            \n            # Store results\n            methods_fi_results[method_name][i] = stacking_output[0]\n            methods_meta_weights[method_name][i] = stacking_output[1]\n            \n            # Get stacking predictions and compute performance metrics\n            stacking_pred = stacking_output[3]\n            methods_scores[method_name]['r2'].append(r2_score(y_test, stacking_pred))\n            methods_scores[method_name]['mse'].append(mean_squared_error(y_test, stacking_pred))\n\n    return methods_fi_results, methods_scores\n\ndef plot_feature_importance_boxplots(methods_fi_results, feature_count, output_dir):\n    \"\"\"Create boxplots to compare feature importance calculation methods\"\"\"\n    plt.figure(figsize=(15, 10))\n    \n    n_features = feature_count\n    \n    # Create subplot grid based on number of features\n    n_cols = 2\n    n_rows = (n_features + 1) // 2  # Ceiling division\n    \n    method_names = ['Method 1', 'Method 2', 'Method 3', 'Method 4']\n    \n    for feat_idx in range(n_features):\n        plt.subplot(n_rows, n_cols, feat_idx + 1)\n        \n        # Extract importance values for this feature across all simulations for each method\n        feat_importances = [methods_fi_results[method][:, feat_idx] for method in methods_fi_results]\n        \n        # Create boxplot\n        plt.boxplot(feat_importances, labels=method_names)\n        plt.title(f'Feature {feat_idx+1} Importance')\n        plt.grid(True, linestyle='--', alpha=0.6)\n    \n    plt.suptitle(f'Stacking Feature Importance Methods Comparison - {n_features} Features', y=1.02, fontsize=16)\n    plt.tight_layout()\n    plt.savefig(output_dir / f'stacking_importance_comparison_f{n_features}.png', dpi=300, bbox_inches='tight')\n    plt.close()\n\ndef compare_importance_methods(methods_fi_results, feature_count, output_dir):\n    \"\"\"Compare different stacking feature importance methods\"\"\"\n    # Calculate mean and standard deviation for each method\n    mean_fi = {method: np.mean(results, axis=0) for method, results in methods_fi_results.items()}\n    std_fi = {method: np.std(results, axis=0) for method, results in methods_fi_results.items()}\n    \n    method_names = list(methods_fi_results.keys())\n    \n    # Create bar chart for comparison\n    plt.figure(figsize=(15, 6))\n    \n    x = np.arange(feature_count)\n    width = 0.2\n    \n    # Plot bars for each method\n    for i, method in enumerate(method_names):\n        plt.bar(x + i*width - width*1.5, mean_fi[method], width, \n                label=f'Method {i+1}', alpha=0.7, \n                yerr=std_fi[method], capsize=5)\n    \n    plt.title(f'Feature Importance Methods Comparison - {feature_count} Features')\n    plt.xlabel('Feature Index')\n    plt.ylabel('Mean Importance')\n    plt.xticks(x, [f'F{j+1}' for j in range(feature_count)])\n    plt.legend()\n    plt.grid(True, linestyle='--', alpha=0.6)\n    \n    plt.tight_layout()\n    plt.savefig(output_dir / f'importance_comparison_f{feature_count}.png', dpi=300)\n    plt.close()\n    \n    # Calculate correlation matrix between methods\n    method_correlations = np.zeros((len(method_names), len(method_names)))\n    for i, method1 in enumerate(method_names):\n        for j, method2 in enumerate(method_names):\n            method_correlations[i, j] = np.corrcoef(\n                mean_fi[method1], mean_fi[method2]\n            )[0, 1]\n    \n    # Create heatmap of method correlations\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(method_correlations, annot=True, cmap='coolwarm', \n                xticklabels=method_names, yticklabels=method_names)\n    plt.title('Correlation Between Feature Importance Methods')\n    plt.tight_layout()\n    plt.savefig(output_dir / 'method_correlations_heatmap.png', dpi=300)\n    plt.close()\n    \n    return method_correlations\n\ndef analyze_stability(methods_fi_results, feature_count, output_dir):\n    \"\"\"Analyze stability of feature importance methods\"\"\"\n    # Calculate coefficient of variation (CV) for each method\n    cv_methods = {}\n    for method, results in methods_fi_results.items():\n        cv_methods[method] = np.std(results, axis=0) / np.mean(results, axis=0)\n    \n    # Create bar chart for stability comparison\n    plt.figure(figsize=(12, 6))\n    \n    x = np.arange(feature_count)\n    width = 0.2\n    \n    method_names = list(methods_fi_results.keys())\n    \n    # Plot CV for each method\n    for i, method in enumerate(method_names):\n        plt.bar(x + i*width - width*1.5, cv_methods[method], width, \n                label=f'Method {i+1}', alpha=0.7)\n    \n    plt.title(f'Stability Comparison (Lower CV = More Stable) - {feature_count} Features')\n    plt.xlabel('Feature Index')\n    plt.ylabel('Coefficient of Variation')\n    plt.xticks(x, [f'F{j+1}' for j in range(feature_count)])\n    plt.legend()\n    plt.grid(True, linestyle='--', alpha=0.6)\n    \n    plt.tight_layout()\n    plt.savefig(output_dir / f'stability_comparison_f{feature_count}.png', dpi=300)\n    plt.close()\n    \n    # Calculate overall stability metrics\n    mean_cv_methods = {method: np.mean(cv) for method, cv in cv_methods.items()}\n    \n    return {\n        'cv_methods': cv_methods,\n        'mean_cv_methods': mean_cv_methods\n    }\n\ndef generate_summary_report(methods_fi_results, methods_scores, method_correlations, stability_metrics, output_dir):\n    \"\"\"Generate a summary report of the findings\"\"\"\n    report_path = output_dir / 'summary_report.txt'\n    \n    with open(report_path, 'w') as f:\n        f.write(\"Stacking Feature Importance Methods Comparison Report\\n\")\n        f.write(\"=================================================\\n\\n\")\n        \n        # Performance Metrics\n        f.write(\"Model Performance Metrics:\\n\")\n        f.write(\"----------------------\\n\")\n        for method, scores in methods_scores.items():\n            f.write(f\"\\n{method}:\\n\")\n            # Calculate mean and std for R² and MSE\n            r2_mean = np.mean(scores['r2'])\n            r2_std = np.std(scores['r2'])\n            mse_mean = np.mean(scores['mse'])\n            mse_std = np.std(scores['mse'])\n            \n            f.write(f\"  Mean R²: {r2_mean:.4f} (±{r2_std:.4f})\\n\")\n            f.write(f\"  Mean MSE: {mse_mean:.4f} (±{mse_std:.4f})\\n\")\n        \n        # Method Correlations\n        f.write(\"\\nFeature Importance Method Correlations:\\n\")\n        f.write(\"------------------------------------\\n\")\n        method_names = list(methods_fi_results.keys())\n        for i, method1 in enumerate(method_names):\n            for j, method2 in enumerate(method_names):\n                if i != j:\n                    f.write(f\"{method1} vs {method2}: {method_correlations[i, j]:.4f}\\n\")\n        \n        # Stability Analysis\n        f.write(\"\\nMethod Stability Analysis:\\n\")\n        f.write(\"------------------------\\n\")\n        for method, mean_cv in stability_metrics['mean_cv_methods'].items():\n            f.write(f\"{method} Mean CV: {mean_cv:.4f}\\n\")\n        \n        # Recommendations\n        f.write(\"\\nRecommendations:\\n\")\n        f.write(\"---------------\\n\")\n        \n        # Find most stable method\n        most_stable_method = min(stability_metrics['mean_cv_methods'], \n                                 key=stability_metrics['mean_cv_methods'].get)\n        \n        f.write(f\"Most stable method: {most_stable_method}\\n\")\n        \n        # Find highest performing method based on R²\n        r2_means = {method: np.mean(scores['r2']) for method, scores in methods_scores.items()}\n        best_method = max(r2_means, key=r2_means.get)\n        \n        f.write(f\"Best performing method (R²): {best_method}\\n\")\n\ndef visualize_and_compare_methods(methods_fi_results, methods_scores):\n    \"\"\"Comprehensive visualization and comparison of methods\"\"\"\n    # Create output directory\n    output_dir = Path('feature_importance_comparison')\n    output_dir.mkdir(exist_ok=True)\n    \n    # Get feature count from results\n    feature_count = list(methods_fi_results.values())[0].shape[1]\n    \n    # Plot feature importance boxplots\n    plot_feature_importance_boxplots(methods_fi_results, feature_count, output_dir)\n    \n    # Compare methods\n    method_correlations = compare_importance_methods(methods_fi_results, feature_count, output_dir)\n    \n    # Analyze stability\n    stability_metrics = analyze_stability(methods_fi_results, feature_count, output_dir)\n    \n    # Generate summary report\n    generate_summary_report(methods_fi_results, methods_scores, \n                            method_correlations, stability_metrics, output_dir)\n\n\ndef run_all_scenarios():\n    \"\"\"Run all scenarios and generate comparisons\"\"\"\n    # Initialize configuration\n    config = SimulationConfig()\n    \n    # Get all possible scenarios\n    scenarios = list(itertools.product(\n        config.n_simulations_options,\n        config.sample_sizes,\n        config.n_features_options,\n        config.friedman_types\n    ))\n    \n    print(f\"Running {len(scenarios)} scenarios...\")\n    \n    # Set up parallel processing\n    n_processes = max(1, mp.cpu_count() - 1)\n    pool = mp.Pool(processes=n_processes)\n    \n    # Run scenarios in parallel\n    results = []\n    \n    try:\n        for i, scenario in enumerate(scenarios):\n            n_sims, sample_size, n_features, friedman_type = scenario\n            \n            try:\n                # Run simulation for all 4 methods\n                methods_fi_results, methods_scores = run_simulation(\n                    n_simulations=n_sims,\n                    n_samples=sample_size,\n                    n_features=n_features,\n                    friedman_type=friedman_type,\n                    config=config\n                )\n                \n                # Prepare result dictionary\n                result = {\n                    'success': True,\n                    'n_simulations': n_sims,\n                    'sample_size': sample_size,\n                    'n_features': n_features,\n                    'friedman_type': friedman_type,\n                    'methods_fi_results': methods_fi_results,\n                    'methods_scores': methods_scores\n                }\n                \n                results.append(result)\n                print(f\"Completed scenario {i+1}/{len(scenarios)}: {n_features} features, {sample_size} samples\")\n            \n            except Exception as e:\n                print(f\"Failed scenario {i+1}/{len(scenarios)}: {str(e)}\")\n                results.append({\n                    'success': False,\n                    'n_simulations': n_sims,\n                    'sample_size': sample_size,\n                    'n_features': n_features,\n                    'friedman_type': friedman_type,\n                    'error': str(e)\n                })\n    \n    except KeyboardInterrupt:\n        print(\"\\nReceived keyboard interrupt, stopping gracefully...\")\n        pool.terminate()\n    finally:\n        pool.close()\n        pool.join()\n    \n    # Generate comparison visualizations\n    successful_results = [r for r in results if r.get('success', False)]\n    \n    if successful_results:\n        print(\"Generating feature importance comparisons...\")\n        \n        # Group results by feature count\n        feature_count_groups = {}\n        for scenario in successful_results:\n            n_features = scenario['n_features']\n            if n_features not in feature_count_groups:\n                feature_count_groups[n_features] = []\n            feature_count_groups[n_features].append(scenario)\n        \n        # Process each feature count group separately\n        for n_features, scenarios_group in feature_count_groups.items():\n            # Aggregate results for this feature count\n            all_methods_fi_results = {}\n            all_methods_scores = {}\n            \n            # Collect results for each method across scenarios with this feature count\n            method_keys = ['method1', 'method2', 'method3', 'method4']\n            for method in method_keys:\n                # Collect feature importance results for this method and feature count\n                method_fi_results = [\n                    scenario['methods_fi_results'][method] \n                    for scenario in scenarios_group\n                ]\n                \n                # Ensure all arrays have the same number of features\n                method_fi_results = [\n                    arr[:, :n_features] if arr.shape[1] > n_features else arr \n                    for arr in method_fi_results\n                ]\n                \n                # Combine results\n                all_methods_fi_results[method] = np.vstack(method_fi_results)\n                \n                # Combine scores\n                all_methods_scores[method] = {\n                    'r2': [score for scenario in scenarios_group \n                           for score in scenario['methods_scores'][method]['r2']],\n                    'mse': [score for scenario in scenarios_group \n                            for score in scenario['methods_scores'][method]['mse']]\n                }\n            \n            # Visualize and compare methods for this feature count\n            visualize_and_compare_methods(all_methods_fi_results, all_methods_scores)\n        \n        print(\"Comparison generation complete.\")\n    else:\n        print(\"No successful scenarios to summarize.\")\n\n# Main execution\nif __name__ == '__main__':\n    try:\n        # Run all scenarios and generate comparisons\n        run_all_scenarios()\n        print(\"Analysis completed. Results are in the 'feature_importance_comparison' directory.\")\n        \n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n        import traceback\n        traceback.print_exc()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-04T21:41:54.231351Z","iopub.execute_input":"2025-03-04T21:41:54.231853Z","iopub.status.idle":"2025-03-04T22:00:44.937431Z","shell.execute_reply.started":"2025-03-04T21:41:54.231817Z","shell.execute_reply":"2025-03-04T22:00:44.935496Z"}},"outputs":[{"name":"stdout","text":"Running 4 scenarios...\n\nReceived keyboard interrupt, stopping gracefully...\nNo successful scenarios to summarize.\nAnalysis completed. Results are in the 'feature_importance_comparison' directory.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}